# 빅데이터

## I. 정의

- 모르겠음... 밑에 거 3가지 정도면 될 듯
  - Volume
    - 페타바이트 단위
    - 분산 컴퓨팅
  - Velocity
    - 실시간처리
    - 분산 컴퓨팅
  - Variety
    - 정형
    - 반정형
    - 비정형?

- 꽁짜!!
- RDBMS 와 상호보완적

## II. 문제

- CO2?



## III. 빅 데이터 전 쓰던 거

- BI
- OLAP
  - 데이터를 실시간으로 처리하는 방법
  - 아직도 많이 쓰고 있다
  - 꼭 빅 데이터만 좋은 게 아님...



## IV. 하둡 설치

- v1.2.1
  - 독립 실행 : 분산 처리 x, 확장 불가
  - 가상 분산 : 가장 일반적, 하나의 서버, 분산 환경 (우린 이거 할 거)
  - 완전 분산



- 하둡을 /etc/ 에 옮기기

```shell
vim /etc/profile

HADOOP_HOME=/etc/hadoop-1.2.1
export HADOOP_HOME
```

- 우리의 목표

```shell
가상 분산 모드 구축

1. NAMENODE
2. SECONDARY NAMENODE
3. DATANODE
4. DATANODE

총 4대의 서버? 를 한 컴퓨터에서 구축해본다.
```



- 머신에서 나갔다가 들어올 때, 비밀번호 검사를 하지 않게 하기 위해서
- 키 페어를 만들어 준다.

```shell
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
```

- 이제 배포용 퍼블릭 키를 만든다.

```shell
cat id_dsa.pub >> authorized_keys
```

- 그럼 이제 비밀번호 안물어봄 ^오^



- 방화벽을... 안쓰는거...?

```shell
sudo systemctl disable firewalld
```



- 하둡의 configuration 파일 수정

```shell
1. core-site.xml
2. hdfs-site.xml
3. mapred-site.xml
```



- core-site.xml

```xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>dfs.tmp.dir</name>
# hadoop.tmp.dir 로 변경바람
        <value>/etc/hadoop-1.2.1/tmp</value>
    </property>
</configuration>

```

- hdfs-site.xml

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.name.dir</name>
        <value>/etc/hadoop-1.2.1/name</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>/etc/hadoop-1.2.1/data</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>
```



- mapred-site.xml

```xml
<configuration>
    <property>
        <name>mapred.job.tracker</name>
        <value>localhost:9001</value>
    </property>
</configuration>
```



- 마지막으로 hadoop-env.sh

```shell
export JAVA_HOME=/etc/jdk1.8
# 왜 자바 또함?
# 또 본대!!

export HADOOP_HOME_WARN_SUPPRESS="TRUE"
```



- 찐막으로 format 을 해줘야 한다.

```shell
hadoop namenode -format
```



- start-all.sh 로 실행시킨다.



## V. 데이터 입력

- 데이터를 datanode 에 넣어보자
  - 70.12.114.242:50070 에 접속
  - 브라우저를 보면 etc 밖에 없다.
  - test 폴더를 생성
  - 데이터를 입력

```shell
hadoop dfs -mkdir /test
```

```shell
hadoop dfs -put README.txt /test
```



## VI. 데이터 분석

- hadoop-example 을 실행해보자

```shell
hadoop jar hadoop-example* wordcount /data/input1 /data/output1
```

