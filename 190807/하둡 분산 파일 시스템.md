# 하둡 분산 파일 시스템

## I. 하둡은

- HDFS(분산 저장) + MAP REDUCE (읽기, 쓰기, 분석) + DISTRIBUTED SYSTEM



## II. 기존 대용량 파일 시스템

- DAS : 서버가 있어서, 서버 하드에 접근
- NAS : 네트워크를 통해 공유..?
- SAN : DAS 나 NAS 를 모아서 총괄?



## III. HDFS

- 저렴
- 중앙집중식이 아닌 퍼짐
- 대규모 데이터, 배치 처리(바로 올라오지 않고 시간을 두고 올라옴)에 강함
  - 트랜잭션 실시간 처리, 안정적인 처리엔 약함

- 장애 복구에 강함
  - Replication 을 통해 디스크가 깨져도 다시 가지고 올 수 있다.
- 스트리밍 방식으로 작동
- 한 번 넣으면 수정이 불가



1. 블록 구조
   - HDFS 는 기본 sector 가 64M 임
   - 바꿀수도 잇음
   - 64M 씩 나눠져서 각 데이터 노드에 들어감 : 빠르다!, 좋다!



## IV. 명령어

```shell
hadoop fs -mkdir data
# /user/root/ 폴더에 만들어진다.
hadoop fs -ls
hadoop fs -lsr

hadoop fs -du
hadoop fs -dus

hadoop fs -cat

hadoop fs -mv
hadoop fs -cp

hadoop fs -chmod
hadoop fs -chown
hadoop fs -chgrp

hadoop fs -touchz

hadoop fs -expunge
```



## V. 하이브-1.0.1

- Jobtracker 요청을 SQL 문처럼 해서 보내주자
- JobTracker 를 좀 더 편리하게 요청해보자



- 요구사항
  - mysql (hive 가 데이터 구조정보를 mysql 에 저장한다)

- 설치
  - mysql 설치
  - hive 계정 생성

```mysql
mysql -u root -p
111111

grant all privileges on *.* to 'hive'@'localhost' identified by '111111';

flush privileges;

create database hive_db;
grant all privileges on hive_db.* to 'hive'@'%' identified by '111111' with grant option;

grant all privileges on hive_db.* to 'hive'@'localhost' identified by '111111' with grant option;

flush privileges;
commit;

mysql -u hive -p hive_db
```

- hive-1.0.1 설치

```shell
cp -r apache-hive /etc/
vim /etc/profile

HIVE_HOME=/etc/apache-hive;
PATH=$HIVE_HOME/bin:$PATH;

export HIVE_HOME PATH
```

- hive 와 hadoop 연동(hive-site.xml)

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>hive.metastore.local</name>
        <value>true</value>
        <description>controls whether to connect to remove metastore server or open a new metastore server in Hive Client JVM</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mariadb://localhost:3306/hive_db?createDatabaseIfNotExist=true</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.mariadb.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
        <description>username to use against metastore database</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>111111</value>
        <description>password to use against metastore database</description>
    </property>
</configuration>
```

- mariadb-java-client-1.3.5.jar 를 lib 폴더에 넣기
- hadoop hive 를 위한 폴더 만들어 주기

```shell
hadoop fs -mkdir /tmp
haddop fs -mkdir /user/root/warehouse

hadoop fs -chmod g+w /tmp
hadoop fs -chmod g+w /user/root/warehouse

hadoop fs -chmod 777 /tmp/hive
```

- hive 로 실행



## VI. 하이브 쓰는 법

- MariaDB 에 log 파일의 구조만 입력한다.

```mysql
CREATE TABLE HDI(id INT, country STRING, hdi FLOAT, lifeex INT, mysch INT, 

eysch INT, gni INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS 

TEXTFILE;
```

- hive 에서 파일을 연동한다.

```hive
load data local inpath '/root/hdi.txt' into table HDI;

# 이러면 문서가 hadoop 에 들어간다.
```

- 우리가 하고 싶은 명령을 SQL 문으로 요청한다.

```mysql
SELECT id, country FROM HDI;
```

